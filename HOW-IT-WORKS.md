# Cortex: How It Works

Cortex is a persistent memory plugin for Claude Code. It automatically learns from your coding sessions and surfaces relevant knowledge when you start new ones.

## The 30-Second Version

```
Session ends → reads transcript → Claude CLI extracts memories → stored in SQLite
Session starts → loads ranked memories → writes to .claude/cortex-memory.local.md
```

Claude Code reads that `.md` file as context, so it "remembers" past sessions.

---

## Two Databases

Cortex maintains **two** SQLite databases:

| Database | Location | Scope |
|---|---|---|
| **Project** | `<project>/.memory/cortex.db` | Project-specific memories (default) |
| **Global** | `~/.claude/memory/cortex-global.db` | Cross-project knowledge |

A memory is stored in **global** only if the LLM assigns it confidence > 0.8 AND classifies its scope as "global" (e.g., "TypeScript generics work like this" vs "our API uses X pattern").

---

## Session Lifecycle

### 1. Session Start (`SessionStart` hook → `load-surface.sh`)

When you start Claude Code:

1. Check for a **cached surface** file (keyed by `sha256(branch:cwd)`)
2. If cache exists and is < 24h old → serve from cache
3. If stale or missing → regenerate from DB (explained in "Surface Generation" below)
4. Write result to `.claude/cortex-memory.local.md`

Claude Code loads this file as context. The surface is ~300-500 tokens — enough for relevant memories without bloating the context window.

### 2. During Session (Manual Commands)

You can interact with cortex manually:

| Command | What it does |
|---|---|
| `/remember` | Manually store a specific memory |
| `/recall <query>` | Search memories by semantic similarity or keywords |
| `/forget <id>` | Mark a memory as archived |
| `/index-code <path>` | Index source files as code memories |
| `/consolidate` | Merge duplicate/overlapping memories |
| `/inspect` | View memory stats, recent extractions, DB health |

### 3. Session End (`Stop` hook → `extract-and-generate.sh`)

When your session ends:

1. **Read transcript** — the JSONL file Claude Code writes during the session
2. **Resume from checkpoint** — if transcript > 100KB, extraction is resumable; picks up where it left off
3. **Send to Claude CLI** — pipes extraction prompt to `claude -p` (uses your Anthropic subscription)
4. **Parse response** — validate each memory candidate (type, confidence, priority)
5. **Store in DB** — insert memories, compute similarity edges to existing memories
6. **Run lifecycle** — decay old memories, archive stale ones, prune ancient ones
7. **Invalidate surface cache** — new memories mean cached surfaces are stale
8. **Regenerate surface** — immediately rebuild so next session starts fresh

---

## What Gets Extracted (The Extraction Prompt)

The LLM receives your full session transcript (or 100KB chunk) along with git context (branch, recent commits, changed files). It's asked to extract memories in 8 categories:

| Type | What it captures | Example |
|---|---|---|
| `architecture` | System design, structure, patterns | "The app uses FC/IS with pure business logic in core/" |
| `decision` | Choices made with rationale | "Chose SQLite over Postgres for single-user embedded use" |
| `pattern` | Reusable code/design patterns | "All API routes follow parse → service → format pattern" |
| `gotcha` | Pitfalls, edge cases, warnings | "vi.mock() at module level leaks between test files" |
| `context` | Background info, explanations | "The SOPS setup uses age keys, not GPG" |
| `progress` | Status updates, completed work | "Completed wave 6, all 729 tests passing" |
| `code_description` | Prose explanation of code | "The ranking formula weights confidence 50%, priority 20%" |
| `code` | Raw source code (paired with descriptions) | Actual code snippets |

Each memory gets:
- **Confidence** (0-1): How clear and actionable is this knowledge?
- **Priority** (1-10): How important is it?
- **Scope**: Project-specific or globally useful?
- **Tags**: Keywords for searchability

---

## How Memories Are Ranked

When generating the surface or returning search results, memories are scored using a composite formula:

```
rank = (confidence × 0.50)
     + (priority/10 × 0.20)
     + (centrality × 0.15)
     + (log(access+1)/maxLog × 0.15)
     + branch_boost (0.1 if same branch)
```

- **Confidence**: LLM's assessment of quality (decays over time)
- **Priority**: LLM's assessment of importance (static)
- **Centrality**: Graph metric — memories connected to many others rank higher
- **Access frequency**: Memories you search for often rank higher (logarithmic to avoid runaway)
- **Branch boost**: Memories from the current git branch get a +0.1 nudge

---

## Surface Generation

The "push surface" is the markdown file Claude reads at session start. It's generated by:

1. Fetch all active memories from both project + global DBs
2. Compute graph centrality for all memories
3. Rank using the formula above
4. Select top memories within **per-category line budgets**:

| Category | Line Budget |
|---|---|
| Architecture | 25 |
| Decision | 25 |
| Pattern | 25 |
| Gotcha | 20 |
| Progress | 30 |
| Context | 15 |
| Code Description | 10 |
| Code | 0 (excluded — too large) |

5. Allow high-value memories to overflow into unused budget from other categories
6. Target 300-500 tokens total
7. Wrap in `<!-- CORTEX_MEMORY_START -->` / `<!-- CORTEX_MEMORY_END -->` markers
8. Cache the result keyed by `sha256(branch:cwd)`

---

## Memory Graph (Similarity Edges)

When new memories are inserted, they're compared to all existing active memories. The system uses a **two-tier similarity** approach, though currently only Jaccard is wired up at insertion time:

### Tier 1: Jaccard Pre-Filter (active)

Cheap token-overlap comparison used for edge creation during extraction:

| Jaccard Score | Pre-filter | Action |
|---|---|---|
| < 0.1 | `definitely_different` | Skip entirely |
| 0.1 - 0.4 | `maybe` | Create `relates_to` edge (strength = score) |
| 0.4 - 0.5 | `maybe` | Create `suggested` edge for review |
| 0.5+ | `maybe` | Flag for consolidation (logged, not auto-merged in v1) |
| 0.6+ | `definitely_similar` | Create strong `relates_to` edge (strength = 0.9) |

### Tier 2: Cosine Similarity on Embeddings (plumbed but not yet wired)

`cosineSimilarity()` exists in `core/similarity.ts` and is used by `/recall` for search ranking. The plan is to use it during edge creation for the "maybe" range (0.1-0.6 Jaccard) once embeddings are backfilled — giving much more accurate similarity than token overlap. Currently, the "maybe" range falls through to Jaccard-only classification.

### Edge Types

`relates_to`, `derived_from`, `contradicts`, `exemplifies`, `refines`, `supersedes`, `source_of`

### Graph Uses

- **Centrality scoring** in surface ranking formula
- **Graph traversal** in `/recall` (depth-2 BFS finds related memories)
- **Consolidation candidates** (high similarity → `/consolidate` can merge them)

---

## Memory Lifecycle (Decay → Archive → Prune)

Memories aren't permanent. After every extraction, the lifecycle runs:

### Decay
Confidence decays over time based on age and access patterns. Highly connected memories (high centrality) decay slower. Pinned memories are exempt.

### Archive
If confidence drops below 0.3 AND the memory hasn't been accessed in 14+ days → status changes to `archived`. Archived memories don't appear in the surface.

### Prune
If a memory has been archived for 90+ days → status changes to `pruned`. Pruned memories are effectively deleted (still in DB but invisible to all queries).

**Escape hatch**: Accessing a memory (via `/recall`) resets its access timestamp and boosts it back above the threshold.

---

## Semantic Search (Recall)

`/recall <query>` searches using two methods:

### Semantic (default, requires `GEMINI_API_KEY`)
1. Embed the query via Gemini Embedding API (`gemini-embedding-001`)
2. Query is prefixed: `[query] [project:name] <your query>`
3. Memories are prefixed: `[memory_type] [project:name] <summary>`
4. Cosine similarity search against stored embeddings in both DBs
5. Results merged (project first), enriched with graph-traversed related memories

### Keyword (fallback, or `--keyword` flag)
- FTS5 full-text search on memory content and summary
- No API key needed
- Less precise but works offline

### Embedding Backfill
Memories are inserted without embeddings (to avoid blocking extraction). A background `backfill` command computes embeddings for all un-embedded memories in batch. This runs periodically or can be triggered manually.

---

## File Layout

```
<project>/
  .memory/
    cortex.db              # Project SQLite database
    surface-cache/         # Cached surface files (branch-keyed)
    locks/                 # PID lock files for concurrent access
    cortex-status.json     # Telemetry (last generation, timing)
  .claude/
    cortex-memory.local.md # The surface file Claude reads

~/.claude/
  memory/
    cortex-global.db       # Global SQLite database
  plugins/
    cortex/
      .claude-plugin/
        plugin.json        # Plugin manifest
      hooks/
        hooks.json         # Hook registrations
        scripts/
          extract-and-generate.sh  # Stop hook
          load-surface.sh          # SessionStart hook
      engine/src/          # TypeScript source
      commands/            # Skill markdown files
```

---

## Environment Variables

| Variable | Purpose | Required |
|---|---|---|
| `GEMINI_API_KEY` | Embedding backfill + semantic search | Yes (for embeddings + semantic recall) |
| `CLAUDE_PLUGIN_ROOT` | Plugin directory (set by Claude Code) | Auto |

Extraction uses `claude -p` (Claude CLI) — no API key needed, uses your Anthropic subscription. The `claude` binary must be on PATH (it is when running inside Claude Code hooks).

Without `GEMINI_API_KEY`, embeddings are skipped and recall falls back to keyword search. Extraction still works via Claude CLI.
